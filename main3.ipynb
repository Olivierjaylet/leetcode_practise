{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import  train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:4: SyntaxWarning: invalid escape sequence '\\k'\n",
      "<>:4: SyntaxWarning: invalid escape sequence '\\k'\n",
      "C:\\Users\\Olivier\\AppData\\Local\\Temp\\ipykernel_11688\\2489054802.py:4: SyntaxWarning: invalid escape sequence '\\k'\n",
      "  df = pd.read_csv('kdt-NLANU-0.01.connlu.txt\\kdt-NLANU-0.01.connlu.txt',\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>WORD</th>\n",
       "      <th>LEMMA</th>\n",
       "      <th>POS</th>\n",
       "      <th>XPOS</th>\n",
       "      <th>MORPH</th>\n",
       "      <th>HEAD</th>\n",
       "      <th>DEPREL</th>\n",
       "      <th>DEPS</th>\n",
       "      <th>MISC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>ҚТЖ</td>\n",
       "      <td>ҚТЖ</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>_</td>\n",
       "      <td>4</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>халықаралық</td>\n",
       "      <td>халықаралық</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>_</td>\n",
       "      <td>3</td>\n",
       "      <td>amod</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>серіктестікті</td>\n",
       "      <td>серіктестік</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>Case=Acc</td>\n",
       "      <td>4</td>\n",
       "      <td>dobj</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>кеңейтуде</td>\n",
       "      <td>кеңей</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VERB</td>\n",
       "      <td>Person=3|vbTense=Aor|vbVcCaus=True</td>\n",
       "      <td>0</td>\n",
       "      <td>root</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>160</td>\n",
       "      <td>160</td>\n",
       "      <td>NUM</td>\n",
       "      <td>NUM</td>\n",
       "      <td>_</td>\n",
       "      <td>2</td>\n",
       "      <td>compound</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ID           WORD        LEMMA    POS   XPOS  \\\n",
       "1  1            ҚТЖ          ҚТЖ  PROPN  PROPN   \n",
       "2  2    халықаралық  халықаралық    ADJ    ADJ   \n",
       "3  3  серіктестікті  серіктестік   NOUN   NOUN   \n",
       "4  4      кеңейтуде        кеңей   VERB   VERB   \n",
       "6  1            160          160    NUM    NUM   \n",
       "\n",
       "                                MORPH HEAD    DEPREL DEPS MISC  \n",
       "1                                   _    4     nsubj    _    _  \n",
       "2                                   _    3      amod    _    _  \n",
       "3                            Case=Acc    4      dobj    _    _  \n",
       "4  Person=3|vbTense=Aor|vbVcCaus=True    0      root    _    _  \n",
       "6                                   _    2  compound    _    _  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = [\"ID\", \"WORD\", \"LEMMA\", \"POS\", \"XPOS\", \"MORPH\", \"HEAD\", \"DEPREL\", \"DEPS\", \"MISC\"]\n",
    "\n",
    "# Read the file and convert it to a DataFrame\n",
    "df = pd.read_csv('kdt-NLANU-0.01.connlu.txt\\kdt-NLANU-0.01.connlu.txt', \n",
    "                 sep='\\t', \n",
    "                 names=columns, \n",
    "                 skip_blank_lines=True)\n",
    "\n",
    "# Drop rows where 'WORD' is NaN\n",
    "df = df.dropna(subset=['WORD'])\n",
    "\n",
    "df = df[df['POS'] != 'PUNCT']\n",
    "\n",
    "# Characters to remove\n",
    "chars_to_remove = r\"[\\#\\$\\%\\&\\(\\)\\+\\,\\-\\.\\–\\’\\:\\@]\"\n",
    "\n",
    "# Removing the characters from the 'WORD' column\n",
    "df['WORD'] = df['WORD'].str.replace(chars_to_remove, '', regex=True)\n",
    "\n",
    "\n",
    "#df['ID'] = df['ID'].astype(int)\n",
    "\n",
    "# Display the first few rows of the cleaned DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_values(df):\n",
    "    X_lex = df['WORD'].str.strip()\n",
    "    X_lex = X_lex.values\n",
    "\n",
    "    y_lex = df['POS'].str.strip()\n",
    "    y_lex = y_lex.values\n",
    "\n",
    "    return X_lex, y_lex\n",
    "df.shape[0] \n",
    "\n",
    "X_lex, Y_lex = get_values(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_lex, Y_lex, test_size=0.1, random_state=42)\n",
    "    \n",
    "#get max word length\n",
    "max_word_len=max(max([len(w) for w in Y_lex]),max([len(w) for w in X_lex]))\n",
    "\n",
    "#Char2vec model\n",
    "vectorizer = TfidfVectorizer(lowercase=False, analyzer='char')\n",
    "X = vectorizer.fit_transform(X_lex)\n",
    "dic=vectorizer.get_feature_names_out()#letter dictionary\n",
    "num_letters=len(dic)\n",
    "mx=X.T.dot(X) #letter cooccurence matrix\n",
    "mx=mx.toarray()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(147, 147)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([' ', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'A', 'B',\n",
       "       'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O',\n",
       "       'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b',\n",
       "       'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o',\n",
       "       'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '³', 'ë',\n",
       "       'Ё', 'І', 'А', 'Б', 'В', 'Г', 'Д', 'Е', 'Ж', 'З', 'И', 'Й', 'К',\n",
       "       'Л', 'М', 'Н', 'О', 'П', 'Р', 'С', 'Т', 'У', 'Ф', 'Х', 'Ц', 'Ч',\n",
       "       'Ш', 'Щ', 'Ы', 'Э', 'Ю', 'Я', 'а', 'б', 'в', 'г', 'д', 'е', 'ж',\n",
       "       'з', 'и', 'й', 'к', 'л', 'м', 'н', 'о', 'п', 'р', 'с', 'т', 'у',\n",
       "       'ф', 'х', 'ц', 'ч', 'ш', 'щ', 'ъ', 'ы', 'ь', 'э', 'ю', 'я', 'ё',\n",
       "       'і', 'Ғ', 'ғ', 'Қ', 'қ', 'ң', 'Ү', 'ү', 'Ұ', 'ұ', 'Һ', 'һ', 'Ә',\n",
       "       'ә', 'Ө', 'ө', '№'], dtype=object)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vec encoding of words\n",
    "def alpha_vec2(w, mx, max_word_len, dic):\n",
    "    vec = np.zeros((max_word_len, len(dic)))    \n",
    "    for i in range(0, len(w)):\n",
    "        vec[i] = mx[np.where(dic == w[i])[0][0]]\n",
    "    vec = vec.astype('float16').flatten()\n",
    "        \n",
    "    vec=vec.astype('float16').flatten()\n",
    "    vec[vec==np.inf]=0 \n",
    "    vec[vec==-np.inf]=0        \n",
    "    return vec\n",
    "\n",
    "\n",
    "\n",
    "#ordinal encoding of words\n",
    "def alpha_vec2ord(w, max_word_len):\n",
    "    vec=np.zeros(max_word_len)    \n",
    "    for i in range(0, len(w)):        \n",
    "        vec[i]=ord(w[i])    \n",
    "    return vec.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Olivier\\AppData\\Local\\Temp\\ipykernel_11688\\467234094.py:6: RuntimeWarning: overflow encountered in cast\n",
      "  vec = vec.astype('float16').flatten()\n"
     ]
    }
   ],
   "source": [
    "#Vectorize\n",
    "X_lex_vec_train=[alpha_vec2(w, mx, max_word_len, dic) for w in X_train]\n",
    "Y_lex_vec_train=[alpha_vec2ord(w, max_word_len) for w in y_train]\n",
    "\n",
    "X_lex_vec_test=[alpha_vec2(w, mx, max_word_len, dic) for w in X_test]\n",
    "Y_lex_vec_test=[alpha_vec2ord(w, max_word_len) for w in y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build model\n",
    "best_model=ExtraTreesClassifier(n_estimators=10, \n",
    "                                n_jobs=5, \n",
    "                                criterion='entropy', \n",
    "                                bootstrap=True, \n",
    "                                verbose=1)\n",
    "\n",
    "best_model.fit(X_lex_vec_train, Y_lex_vec_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Test\n",
    "predicts_test=best_model.predict(X_lex_vec_test)\n",
    "predicts_train=best_model.predict(X_lex_vec_train)\n",
    "test_acc=sum([sum(p==y)==max_word_len for p,y in zip(predicts_test, Y_lex_vec_test)])/len(predicts_test)\n",
    "train_acc=sum([sum(p==y)==max_word_len for p,y in zip(predicts_train, Y_lex_vec_train)])/len(predicts_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
